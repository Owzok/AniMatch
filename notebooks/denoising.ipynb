{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Martin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/10k_users.csv\")[[\"user_id\", \"anime_id\", \"rating\"]]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.user_id.unique())\n",
    "len(df.anime_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = df.groupby(['user_id', 'anime_id'])['rating'].mean().unstack(fill_value=0)\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix.to_pickle(\"useritem.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = pd.read_pickle(\"useritem.pkl\")\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_user_item_matrix = pd.DataFrame(index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "\n",
    "for idx in user_item_matrix.index:\n",
    "    row_values = user_item_matrix.loc[idx]\n",
    "    min_value = row_values.min()\n",
    "    max_value = row_values.max()\n",
    "    normalized_values = row_values.apply(lambda x: 0 if x == 0 else (x - min_value) / (max_value - min_value) * 2 - 1)\n",
    "    user_item_matrix.loc[idx] = normalized_values\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "normalized_user_item_matrix_array = user_item_matrix.to_numpy()\n",
    "normalized_user_item_matrix_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix.to_pickle(\"scaled_item-user.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = pd.read_pickle(\"scaled_item-user.pkl\")\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entries = user_item_matrix.size\n",
    "zero_entries = (user_item_matrix == 0).sum().sum()\n",
    "\n",
    "sparsity_percentage = (zero_entries / total_entries) * 100\n",
    "\n",
    "print(f\"Sparsity of the matrix: {sparsity_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ratings(matrix, percentage=0.25):\n",
    "    \"\"\"\n",
    "    Remove a specified percentage of non-zero ratings from each row of the user-item matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix: pandas DataFrame\n",
    "        The user-item matrix.\n",
    "    - percentage: float, optional (default=0.25)\n",
    "        The percentage of non-zero ratings to remove.\n",
    "\n",
    "    Returns:\n",
    "    - modified_matrix: pandas DataFrame\n",
    "        The modified user-item matrix.\n",
    "    \"\"\"\n",
    "    for idx in matrix.index:\n",
    "        row_values = matrix.loc[idx]\n",
    "\n",
    "        non_zero_indices = row_values[row_values != 0].index\n",
    "\n",
    "        num_ratings_to_remove = int(len(non_zero_indices) * percentage)\n",
    "        ratings_to_remove = np.random.choice(non_zero_indices, size=num_ratings_to_remove, replace=False)\n",
    "\n",
    "        matrix.loc[idx, ratings_to_remove] = 0\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = remove_ratings(user_item_matrix, percentage=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entries = user_item_matrix.size\n",
    "zero_entries = (user_item_matrix == 0).sum().sum()\n",
    "\n",
    "sparsity_percentage = (zero_entries / total_entries) * 100\n",
    "\n",
    "print(f\"Sparsity of the matrix: {sparsity_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix.to_pickle(\"denoised_item-user.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_matrix = pd.read_pickle(\"scaled_item-user.pkl\")\n",
    "noisy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting part comes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 4000)              18792000  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4000)              16004000  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4000)              16004000  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4697)              18792697  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69592697 (265.48 MB)\n",
      "Trainable params: 69592697 (265.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4000, input_shape=(4697,) ,activation=\"tanh\"))\n",
    "model.add(layers.Dense(4000, activation=\"tanh\"))\n",
    "model.add(layers.Dense(4000, activation=\"tanh\"))\n",
    "model.add(layers.Dense(4697, activation=\"linear\")) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    weights = tf.where(tf.equal(y_true, 0), 0.1, 1.0)\n",
    "    loss = tf.reduce_sum(tf.square(y_true - y_pred) * weights) / tf.reduce_sum(weights)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = pd.read_pickle(\"scaled_item-user.pkl\").to_numpy()\n",
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_matrix = pd.read_pickle(\"scaled_item-user.pkl\").to_numpy()\n",
    "noisy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(matrix, batch_size):\n",
    "    num_samples = matrix.shape[0]\n",
    "    while True:\n",
    "        indices = np.random.choice(num_samples, size=batch_size, replace=False)\n",
    "        batch_data = matrix[indices, :]\n",
    "        yield batch_data, batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "325/325 [==============================] - 496s 2s/step - loss: 0.0162\n",
      "Epoch 2/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0073\n",
      "Epoch 3/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0046\n",
      "Epoch 4/50\n",
      "325/325 [==============================] - 483s 1s/step - loss: 0.0034\n",
      "Epoch 5/50\n",
      "325/325 [==============================] - 483s 1s/step - loss: 0.0026\n",
      "Epoch 6/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0019\n",
      "Epoch 8/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0016\n",
      "Epoch 9/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0016\n",
      "Epoch 10/50\n",
      "325/325 [==============================] - 483s 1s/step - loss: 0.0014\n",
      "Epoch 11/50\n",
      "325/325 [==============================] - 483s 1s/step - loss: 0.0015\n",
      "Epoch 12/50\n",
      "325/325 [==============================] - 483s 1s/step - loss: 0.0015\n",
      "Epoch 13/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.7147\n",
      "Epoch 14/50\n",
      "325/325 [==============================] - 486s 1s/step - loss: 0.0694\n",
      "Epoch 15/50\n",
      "325/325 [==============================] - 485s 1s/step - loss: 0.0697\n",
      "Epoch 16/50\n",
      "325/325 [==============================] - 485s 1s/step - loss: 0.0662\n",
      "Epoch 17/50\n",
      "325/325 [==============================] - 485s 1s/step - loss: 0.0598\n",
      "Epoch 18/50\n",
      "325/325 [==============================] - 484s 1s/step - loss: 0.0601\n",
      "Epoch 19/50\n",
      "325/325 [==============================] - 485s 1s/step - loss: 0.0618\n",
      "Epoch 20/50\n",
      "325/325 [==============================] - 501s 2s/step - loss: 0.0629\n",
      "Epoch 21/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0650\n",
      "Epoch 22/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0720\n",
      "Epoch 23/50\n",
      "325/325 [==============================] - 506s 2s/step - loss: 0.0668\n",
      "Epoch 24/50\n",
      "325/325 [==============================] - 507s 2s/step - loss: 0.0671\n",
      "Epoch 25/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0669\n",
      "Epoch 26/50\n",
      "325/325 [==============================] - 507s 2s/step - loss: 0.0675\n",
      "Epoch 27/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0739\n",
      "Epoch 28/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0673\n",
      "Epoch 29/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0673\n",
      "Epoch 30/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0664\n",
      "Epoch 31/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0674\n",
      "Epoch 32/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0657\n",
      "Epoch 33/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0684\n",
      "Epoch 34/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0661\n",
      "Epoch 35/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0648\n",
      "Epoch 36/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0643\n",
      "Epoch 37/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0636\n",
      "Epoch 38/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0639\n",
      "Epoch 39/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0631\n",
      "Epoch 40/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0628\n",
      "Epoch 41/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0632\n",
      "Epoch 42/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0625\n",
      "Epoch 43/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0632\n",
      "Epoch 44/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0627\n",
      "Epoch 45/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0633\n",
      "Epoch 46/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0620\n",
      "Epoch 47/50\n",
      "325/325 [==============================] - 508s 2s/step - loss: 0.0625\n",
      "Epoch 48/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0640\n",
      "Epoch 49/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0618\n",
      "Epoch 50/50\n",
      "325/325 [==============================] - 509s 2s/step - loss: 0.0618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ff491a4bb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_generator(noisy_matrix, batch_size), steps_per_epoch=len(noisy_matrix)//batch_size, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./7hours.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10173/10173 [==============================] - 248s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "reconstructed_data = model.predict(noisy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325509, 4697)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"reconstruction.npy\", reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"noisy.npy\", noisy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
